from pandas import read_csv
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import chi2
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import itertools
from itertools import product
from sklearn.metrics import roc_curve
from sklearn.model_selection import cross_val_score

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def plt_preparation(Y_test, result):
    font = {'size': 15}
    plt.rc('font', **font)

    cnf_matrix = confusion_matrix(Y_test, result)
    plt.figure(figsize=(10, 8))
    plot_confusion_matrix(cnf_matrix, classes=['Legitimate', 'Malware'],
                          title='Confusion matrix')
    plt.savefig("conf_matrix.png")
    plt.show()

def print_curv(Y_test, result):
    sns.set(font_scale=1.5)
    sns.set_color_codes("muted")

    plt.figure(figsize=(10, 8))
    fpr, tpr, thresholds = roc_curve(Y_test, result, pos_label=1)
    lw = 2
    plt.plot(fpr, tpr, lw=lw, label='ROC curve ')
    plt.plot([0, 1], [0, 1])
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC curve')
    plt.savefig("ROC.png")
    plt.show()


def print_best_features(keys, fit_scores):
    key_with_score_list = []
    for i in range(len(fit_scores)):
        key_with_score_list.append([fit_scores[i], keys[i+1]])
    key_with_score_list.sort(reverse=True)
    #print('Check:', key_with_score_list[0:50])
    for i in range(50):
        print(key_with_score_list[i])

def Naive_Byess(X_learn, Y_learn, X_test, Y_test, with_diagram):
    clf = GaussianNB()
    print('Start BS model learn...')
    clf.fit(X_learn, Y_learn)
    print('End BS model leran...')

    result = clf.predict(X_test)
    print('Accuracy of model:', accuracy_score(Y_test, result))
    if with_diagram == 'y':
        plt_preparation(Y_test, result)
        print_curv(Y_test, result)



def Svm(X_learn, Y_learn, X_test, Y_test, with_diagram):
    clf = svm.SVC()
    clf.fit(X_learn, Y_learn)
    print('Start Svm model learn...')
    clf.fit(X_learn, Y_learn)
    print('End Svm model leran...')

    result = clf.predict(X_test)
    print('Accuracy of model:', accuracy_score(Y_test, result))
    if with_diagram == 'y':
        plt_preparation(Y_test, result)
        print_curv(Y_test, result)

def DesTree(X_learn, Y_learn, X_test, Y_testm, with_diagram):
    clf = DecisionTreeClassifier()
    clf.fit(X_learn, Y_learn)
    print('Start DT model learn...')
    clf.fit(X_learn, Y_learn)
    print('End DT model leran...')

    result = clf.predict(X_test)
    print('Accuracy of model:', accuracy_score(Y_test, result))
    if with_diagram == 'y':
        plt_preparation(Y_test, result)
        print_curv(Y_test, result)

def Knn(X_learn, Y_learn, X_test, Y_test, with_diagram):
    clf = KNeighborsClassifier()
    print('Start KNN model learn...')
    clf.fit(X_learn, Y_learn)
    print('End KNN model leran...')

    result = clf.predict(X_test)
    print('Accuracy of model:', accuracy_score(Y_test, result))
    if with_diagram == 'y':
        plt_preparation(Y_test, result)
        print_curv(Y_test, result)

def CrossValTest(X_learn, Y_learn):
    bs_clf = GaussianNB()
    svm_clf = svm.SVC()
    dt_clf = DecisionTreeClassifier()
    knn_clf = KNeighborsClassifier()

    print('BS crosval begin...')
    bs_crosval = cross_val_score(bs_clf, X_learn, Y_learn, cv = 15)
    print('Bs crosval end...')
    print('SVM crosval begin...')
    svm_crosval = cross_val_score(svm_clf, X_learn, Y_learn, cv = 15)
    print('SVM crosval end...')
    print('DT crosval begin...')
    dt_crosval = cross_val_score(dt_clf, X_learn, Y_learn, cv = 15)
    print('DT crosval end...')
    print('KNN crosval begin...')
    knn_crosval = cross_val_score(knn_clf, X_learn, Y_learn, cv = 15)
    print('KNN crosval end...')

    print('BS crosval array', bs_crosval)
    print('BS crosval:', bs_crosval.mean())

    print('SVM crosval array', svm_crosval)
    print('SVM crosval:', svm_crosval.mean())

    print('DT crosval array', dt_crosval)
    print('DT crosval:', dt_crosval.mean())

    print('KNN crosval array', knn_crosval)
    print('KNN crosval:', knn_crosval.mean())

number_of_all_features = 1000
number_of_best_features = 500


path_to_learn_dataset = 'C:\\Users\\nasib\\OneDrive\\Рабочий стол\\Курсовая работа\\top_1000_pe_imports.csv\\top_1000_pe_imports1.csv'
#path_to_test_dataset = 'C:\\Users\\nasib\\OneDrive\\Рабочий стол\\Курсовая работа\\top_1000_pe_imports.csv2\\test.csv'
path_to_test_dataset = 'C:\\Users\\nasib\\OneDrive\\Рабочий стол\\Курсовая работа\\DataSet.csv'




learn_dataset = read_csv(path_to_learn_dataset)
test_dataset = read_csv(path_to_test_dataset)

array_learn = learn_dataset.values
array_test = test_dataset.values



X_learn = array_learn[:,1:number_of_all_features]
X_test = array_test[:,1:number_of_all_features]
X_learn = X_learn.astype('int')
X_test = X_test.astype('int')

Y_learn = array_learn[:,number_of_all_features+1]
Y_test = array_test[:,number_of_all_features+1]
print (Y_learn)

func_learn = SelectKBest(score_func=f_classif, k=number_of_best_features)
Y_learn = Y_learn.astype('int')
Y_test = Y_test.astype('int')


fit = func_learn.fit(X_learn, Y_learn)

print_best_features(learn_dataset.keys(),func_learn.scores_)

X_learn_most = fit.transform(X_learn)
X_test_most = fit.transform(X_test)

crosval = input('CrossVal?(y/n)')

if crosval != 'y':
    with_diagram = input('With diagram?(y/n)')
    Naive_Byess(X_learn_most, Y_learn, X_test_most, Y_test, with_diagram)
    Svm(X_learn_most, Y_learn, X_test_most, Y_test, with_diagram)
    DesTree(X_learn_most, Y_learn, X_test_most, Y_test, with_diagram)
    Knn(X_learn_most, Y_learn, X_test_most, Y_test, with_diagram)
else:
    CrossValTest(X_learn_most, Y_learn)
